// Simulating multiple self-adaptive systems in multi-cloud environment
// by Azlan Ismail
// features: 
// a) one coordinator with loop
// b) two planners
// c) two qosObservers
  

smg

// Player definition that simulates the planning phase
player am1 
adaptPlanner1, qosObserver1, [plan1], [observe1]

endplayer

player am2 
adaptPlanner2, qosObserver2, [plan2], [observe2] 
endplayer

player coord
coordinator, [initiate1], [initiate2], [terminate1], [terminate2]
endplayer

//====================================================
//coordinator
const int CYCLEMAX;
const TEST;
module coordinator
	// State of coordination
	// 0 = both are waiting
	// 1,2 = am1 plans and observes
	// 3,4 = am2 plans and observes
	// 5,6 = am1 and am2 terminates
	s: [0..6] init 0; //for both players
	ctr1: [0..CYCLEMAX] init 0; //limit attempt for am1
	ctr2: [0..CYCLEMAX] init 0; //limit attempt for am2
	
	//coordinate the moves
	[initiate1] (s=0 & p1 <= 3) & (s=0 & ctr1 <= CYCLEMAX) -> (s'=1)&(ctr1'=ctr1+1);   
	[initiate2] (s=0 & p2 <= 3) & (s=0 & ctr2 <= CYCLEMAX) -> (s'=3)&(ctr2'=ctr2+1);   
	[terminate1] (s=0 & p1 > 3) | (s=0 & ctr1 > CYCLEMAX) -> (s'=5);  
	[terminate2] (s=0 & p2 > 3) | (s=0 & ctr2 > CYCLEMAX) -> (s'=6);  

	//am1 moves
	[plan1]    s=1 -> (s'=2); 
	[observe1] s=2 -> (s'=0); 

	//am2 moves
	[plan2]    s=3 -> (s'=4); 
	[observe2] s=4 -> (s'=0); 
endmodule

//======================================================
// am1
module adaptPlanner1
	// State of planning process
	// 0 = input received & reasoning options
	// 1 = action A is selected (App layer)
	// 2 = action B is selected (Infra layer)
        // 3 = action C is selected (Both layers)
	i1 : [0..3] init 0; 

	//select action A, B or C
	[plan1] i1=0 -> 1.5/4:(i1'=1) +1.5/4:(i1'=2) + 1/4:(i1'=3);	
	[initiate1] i1=0 | i1=1 | i1=2 | i1=3 -> (i1'=0);
endmodule

module qosObserver1
	// State of QoS for application layer
	// 0 = no status
	// 1,2,3 = violated
	// 4,5 = healthy
	p1: [0..5] init 0; 

	//estimated QoS effect for application layer
	[observe1] p1=0 -> 1/8:(p1'=1) + 1/8:(p1'=2) + 1/8:(p1'=3) 
                   + 2/8:(p1'=4) + 3/8:(p1'=5);
	[initiate1] p1=0 | p1=1 | p1=2 | p1=3 | p1=4 | p1=5 -> (p1'=0);

endmodule

//=========================================================================
// am2
module adaptPlanner2 = adaptPlanner1 [
	i1=i2, 
	plan1=plan2,
	initiate1=initiate2
] 
endmodule

module qosObserver2 = qosObserver1 [
	p1=p2, 
	observe1=observe2,
	initiate1=initiate2
] 
endmodule

//=============================================
//defining a set of values for the risks
const double init_risk = 0.0;
const double act_r1 = 0.1;
const double act_r2 = 0.2;
const double act_r3 = 0.3;

const double env_r1 = 0.5;
const double env_r2 = 0.4;
const double env_r3 = 0.3;
const double env_r4 = 0.2;
const double env_r5 = 0.1;
const double env_r6 = 0.0;

//=============================================
//formula for am1
formula actRisk_am1 = 	i1=1 ? act_r1 :
			(i1=2 ? act_r2 :
			(i1=3 ? act_r3 : init_risk));

formula envRisk_am1 = 	p1=1 ? env_r1 :
			(p1=2 ? env_r2 :
			(p1=3 ? env_r3 : 
			(p1=4 ? env_r4 :
			(p1=5 ? env_r5 :
			(p1=6 ? env_r6 : init_risk)))));

formula estRisk_am1 = actRisk_am1 + envRisk_am1 + actRisk_am2;

//==============================================
//Estimated risk of planning for am1
//s=2 is the state where a plan is obtained for am1
rewards "allRisk_am1"
	s=2 : estRisk_am1;
endrewards


//=============================================
//formula for am2
formula actRisk_am2 = 	i2=1 ? act_r1 :
			(i2=2 ? act_r2 :
			(i2=3 ? act_r3 : init_risk));

formula envRisk_am2 = 	p2=1 ? env_r1 :
			(p2=2 ? env_r2 :
			(p2=3 ? env_r3 : 
			(p2=4 ? env_r4 :
			(p2=5 ? env_r5 :
			(p2=6 ? env_r6 : init_risk)))));

formula estRisk_am2 = actRisk_am2 + envRisk_am2 + actRisk_am1;

//============================================
//Estimated risk of planning for am2
//s=4 is the state where a plan is obtained for am2
rewards "allRisk_am2"
	s=4 : estRisk_am2;
endrewards

//=============================================
//Risk of selecting the action for am1
rewards "actionRisk1"
	i1=1 : act_r1; i1=2 : act_r2; i1=3 : act_r3;
endrewards

//=============================================
//Risk on the system environment for am1
rewards "envRisk1"
        p1=0 : env_r1; p1=1 : env_r2; p1=2 :env_r3;
	p1=3 : env_r4; p1=4 : env_r5; p1=5 :env_r6;
endrewards

//=============================================
//Risk of selecting the action for am2
rewards "actionRisk2"
	i2=1 : act_r1; i2=2 : act_r2; i2=3 : act_r3;
endrewards

//=============================================
//Risk on the system environment for am2
rewards "envRisk2"
        p2=0 : env_r1; p2=1 : env_r2; p2=2 : env_r3;
	p2=3 : env_r4; p2=4 : env_r5; p2=5 : env_r6;
endrewards
//==============================================

// Labels
label "done" = s=5 | s=6;